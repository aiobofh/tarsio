::

                 _______          _____ ___        ______
                    |      ||    |         |    | |      |
                    |      ||    |         |    | |      |
                    |   ___||___ |         |___ | |______|

                      Copyleft AiO Secure Teletronics


This is Tarsio
--------------

Tarsio is mainly developed to provide an auto-mocking, super-fast platform and
compiler agnostic unit testing framework for C-coders who find Test-Driven
Design a very useful tool.

... Wait, what... Let's go through that again, but slower.


Auto-mocking
^^^^^^^^^^^^

The Tarsio tool-chain provide means to automatically generate mock-up functions
for all functions/library calls you might use in your programs. It mimics the
function signature of every single function that is used and replace the calls
to the original functions and instead a kind-of controllable proxy function is
used instead. This is a mock-up. The programmer can then choose to just control
the return values (if applicable) or check that functions within your other
functions were called a specific number of times, and with what arguments. This
is done in a unit-test.


Super-fast
^^^^^^^^^^

Well... Maybe not ultra-fast, but super-fast. Tarsio rely on your compiler being
able to produce a source-file that is a pre-processed version of your design
under test. This means all pre-processor directives has been sorted out and only
pure C-code remain. Most compilers support this output format. Then in one pass
the Tarsio tool-chain is able to do all its magic by scanning source-files and
test cases to generate a test-runner that execute all your tests within a suite
and keep track of test verdicts.


Platform agnostic
^^^^^^^^^^^^^^^^^

The idea behind this is that most tools of this sort have quite a few
dependencies to other, 3rd party tools. These are not always available on the
platform you intend to work with, or are difficult to compile or port to that
specific system. Tarsio is written to compile with most C compilers, following
pretty old C standards and very few system calls to frameworks usually enjoyed
in the OS. This makes it highly portable.


Compiler agnostic
^^^^^^^^^^^^^^^^^

Since Tarsio operate on source-code level with figuring out what code is needed
to be generated to make life easier for you as a programmer. There are no binary
inspection, and no need for specific features in your favorite compiler, only
that is supports outputting a pre-processed version of your source code and that
the code is in a neat way. Too much poking around in the backwaters of your
compiler feature set will probably make things harder for Tarsio (and you) to
read your code.


Test-Driven Design
^^^^^^^^^^^^^^^^^^

What is this wizardry!? One might ask. Or just wonder if I, the author, have a
spelling error on the word "development". Well. It is neither black magic, nor
a spelling error. There is such impact on how the code will look and be
organized using Test-Driven Development as a design tool. That is why the word
"Design" is chosen instead.


Components and artifacts
------------------------

This a simplified picture on what's going on and what artifacts and tools your
build system would need to take into account when compiling an executable test
runner from your C-source and Unit-tests::

 .-------------.   .------------.   .---------------.
 |  C-source   |   | C-compiler |   | Pre-processed |
 |             |-->|            |-->|               |
 |'Your design'|   |            |   |   C-source    |
 '-------------'   '------------'   '---------------'
                                            |
                                            V
 .-------------.                    .----------------.
 | Unit-tests  |                    | Tarsio command |
 |             |------------------->|   line tools   |
 |  C-source   |                    |   dark magic   |
 '-------------'                    '----------------'
                                        | |    | |
        .-------------------------------' |    | '---------.
        |                .----------------' .--'           |
        |                |                  |              |
        V                V                  V              V
 .-------------.   .------------.   .-------------.  .------------.
 | Mock-ups of |   | Storestruct|   | Test runner |  |  Modified  |
 | all function|   | for every  |   |    code     |  |            |
 | signatures  |   | mock-up    |   |             |  |  C-source  |
 '-------------'   '------------'   '-------------'  '------------'
        |                |                 |               |
        |                '-----. .---------'               |
        '--------------------. | | .-----------------------'
                             | | | |
                             V V V V
 .-----------------.     .--------------.
 | tarsio run-time |---->|  C-compiler  |
 |     library     |     |  and linker  |
 '-----------------'     '--------------'
                                |
                                V
                         .--------------.
                         |  Executable  |
                         | program with |
                         |  test-suite  |
                         '--------------'

Writing a test
--------------

A unit-test in the Tarsio world is supposed to be a close to 100% isolated test
for the code that is the design under test. If Test-Driven Design is a new
concept - Please take a look in "The idea about Test-Driven Design".

In-between every test-case the storage data structure containing samples from
functino call counters, argument monitors and function replacements (stubs) are
resetted (set to zero). The thinking is that this enables every test to be
written as a state-less test. And ther should not be a need to run tests in any
specific order. When the state is cleared it also means that every function call
you have in your code is replaced by a call to a mock-up function (mock).

The Tarsio tool chain supports yet another type of test - They are called
module-test. These are in essence exatly the same as a unit-test but with the
significant difference of clearing the sample storage state, but by default
setting all functions to be called as originally intended intead of just calling
the mock-up. To keep things on a basic level for now - Let's say if


Organizing the files
^^^^^^^^^^^^^^^^^^^^

The idea is to have a 1:1 mapping of test-suite (a file containing a bunch of
test-cases) and the file containing the functions to be tested. The build system
shipped with Tarsio also assumes that the files are named in a specific way. This
is however not a requirement for the Tarsio tool-chain it self. But it might be
a convenient way to keep track on what test-suite is testing what code file.

Consider the case of having a source code file that is going to contain helper
funcitons for disk operations. To easily know what code is in the file by just
browsing the file tree, lets call the file ``disk_operations.c``. Then there
should be a matching test-suite called ``disk_operations_test.c`` containing the
test-cases. Again: This is given that the build systems shipped with Tarisio are
used.


Important includes
^^^^^^^^^^^^^^^^^^

When writing a new test-suite there are a few ``#include``-statements that are
mandatory. First of all you should ``#include`` the ``tarsio.h`` API, which
provide you with the funcitons and macros needed by the Tarsio tool chain.

Also... You probably want to include the ``disk_operations_data.h`` to get
access to all data types and function prototypes used in your design under test.
The ``disk_operations_data.h`` header file is generated by the ``tsg`` tool from
the Tarsio tool chain. See the manual for ``tsg`` for more information. This
filename can be whatever you wish, but in the case of the build-systems shipped
with Tarsio they named like this for convenience, and it's probably a good
practice to do so.

So you should probably have a file starting with the following lines::

  #include <tarsio.h>
  #include "disk_operations_data.h"

Some clarification is needed here. Your text-editor or Integrated Development
Environment may become a bit sad by this inclusion of the generated _data.h
file. Since it might not always exsist... If your editor has some kind of simple
syntax validation some keywords used in your code might be marked as unavailable
or syntax errors. There are ways around this, for example if you generate the
file at some point (which hopefully will be every minute or so, when you get the
hang of Test-Driven Design). Then the text-editor should be a happy camper, most
of the time.


The simplest test
^^^^^^^^^^^^^^^^^

A unit-test is defined by a macro that looks similar to a function prototype or
function header. The macro is called ``test()``, or in a module-test it is called
``module_test()``, depending on which mocking behaviour is desired::

 test(this_is_a_readable_test_name) {
   :
   My test code
   :
 }

A word of warning regarding test-names. Even though it is a very good idea to
name the tests to something valuable and understandable; some C compilers might
have constraints on the length of function names - Usually they are truncaded in
this case, without warning, hence it's quite important to be aware of this.
Mainly since the usual pattern is to prefix the test-case name with the function
that is tested, and then some meaningful description of the test. Given this
knowledge, and if your funcitons under test them selves have meaningful long
names... All the tests might potentially end up being named the exact same thing
due to this truncation.

The number of lines of a unit-test often reflects on the complexity of the code
you are designing. It is a good thing to be aware of this basic rule of thumb:
If your test case is longer than 10-15 lines, your design under test should
probably be refactored, broken apart into smaller testable items.

Again: See the full examples in "The idea about Test-Driven Design" on how to
incrementally build your application using the tests as your development
environment.


Compile'n'run
^^^^^^^^^^^^^

Once your test is ready to be compiled the first time. Just generate the needed
files and compile the test-runner. With the build system shipped with Tarsio this
is done by just building the ``check`` target. For example on a Linux machine
(and many others) in a console just type::

  $ make check

Or even::

  $ make

If you have applied the directory structure in such way that you have a special
folder for tests, that is separated from the code. You choose yourself in your
own build recipes.


The idea about Test-Driven Design
---------------------------------

If you have heard of test-driven development, and even better if you have
practiced it. You probably know what is intended with this semantic game of
changing development to design. If you are new to the concept. Please take your
time to think about what the underlying ideas of this programming paradigm are
trying to achieve.

Tarsio was created to make it as easy as possible to practice and hopefully
enjoy seeing your hack/application/game, or whatever you spend your time with
writing, evolve by writing your code test-driven.

Writing tests first::

         .---------------.                    .-------------------.
         |               V                    |                   V
   .------------.  .------------.     .----------------.  .----------------.
 .-| Write test |  | Write code |---->| Re-factor test |  | Re-factor code |-.
 | '------------'  '------------'     '----------------'  '----------------' |
 |       ^               |                    ^                   |          |
 |       '---------------'                    '-------------------'          |
 |                                                                           |
 |                           .-------------.                                 |
 '---------------------------| Delete test |<--------------------------------'
                             '-------------'

This might be a typical work-flow when always writing tests first, then implement
the code to make the test pass.

Tarsio is helping a programmer to design tests that are very small, and in that
way driving the design of the code.

Imagine an example where you are writing an application that is going to save
some kind of data to disk. This is a perfect example where removal of the
actual file access replacing it with mock-up functions or stubs is highly
recommended to both reduce test complexity, and run-time.

1. Write a test that makes sure that the function that is about to be
   implemented is opening the correct file for writing::

     test(write_file_should_open_the_correct_file_for_writing) {
       write_file("some_file_path.dat");
       assert_eq(1, tarsio_data.fopen.call_count);
       assert_eq(0, strcmp("some_file_path.dat", tarsio_data.fopen.args.arg0);
       assert_eq(0, strcmp("w", tarsio_data.fopen.args.arg1);
     }

2. Compile'n'run

   It will fail, since the write_file() function is not even implemented yet.

3. Implement the code

   A very naive implementation::

     void write_file(const char* filename) {
       (void)fopen(filename, "w");
     }

   The function call to fopen() will automatically be replaced by a call to a
   generated function by the Tariso tools chain, hence we can measure how many
   calls we had to it, and what arguments were passed to it in the generated
   data-storage struct instance ``tarsio_data`` as the test case suggests.

   Something important to be aware of is that return values of function calls
   in a Tarsio processed file always return 0 - This makes it possible to write
   code and tests a bit cleverly, depending on which API is used in the code.
   Many C API's return 0 on success and a negative value on failure. Hence
   the program flow will ripple down through the code i many cases.

4. Compile'n'run

   The test shall now pass, even though this specific test might look very
   trivial and unnecessary it is an enabler for further design of the code.
   Especially making sure that other tests can be written with the knowledge
   that fopen is called correctly and can deal with various errors.

5. Write a test that makes it easy to know what is going wrong with the code
   if a file could not be opened for writing::

     test(write_file_shall_return_negative_1_if_fopen_failes) {
       assert_eq(-1, write_file("some_file_path.dat"));
     }

   Very compact, right... Since the test-suite runner that is generated by the
   Tarsio tool chain clears the data-storage struct ``tarsio_data`` inbetwee
   every test-case, it also clears the ``retval`` member. In this case NULL or
   0.

   While we're at it a test for the normal return value could also be useful
   to drive the test. Let's say that 0 is "everything is OK" return value.

   Now the manipulation of the ``tarsio_data`` storage struct is needed to
   make the call to ``fopen()`` return something known::

     test(write_file_shall_return_0_if_everything_is_ok) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       assert_eq(0, write_file("some_file_path.dat"));
     }

6. Compile'n'run

   This will not even compile, since the original code implementation did not
   have a return-value, since it was a ``void`` function. So take this into
   consideration when implementing the code that will return ``-1`` if the
   call ``fopen()`` fails.

7. Implement the code

   One way of writing it::

     int write_file(const char* filename) {
       if (NULL == fopen(filename, "w")) {
         return -1; /* Could not open file for writing */
       }
       return 0; /* Everything is OK */
     }

   or::

     int write_file(const char* filename) {
       int retval = 0;
       if (NULL == fopen(filename, "w")) {
         retval = -1; /* Could not open file for writing */
       }
       return retval; /* Everything is OK */
     }

   or even::

     int write_file(const char* filename) {
       int retval = 0;
       if (NULL == fopen(filename, "w")) {
         retval = -1;
         goto fopen_failed;
       }
       goto everything_is_ok;

      fopen_failed:
      everything_is_ok:
       return retval;
     }

   The code is still valid and as a programmer you are free to use any style
   you can come up with. The different styles have different charms and
   underlying religious. Tarsio does not care - Just make it readable and
   easy to refactor, self-documenting or whatever you feel like.

8. Compile'n'run

   The tests shall now pass. As you can see, they execute extremely fast,
   since the actual code writing the file to disk is not even called and the
   program flow can now be controlled from the test-cases with very few lines
   of code.

9. Write a few tests to make sure that the *correct* file handle is closed

   ... And only if it actually was opened::

     test(write_file_shall_close_the_correct_file_if_opened) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       write_file("some_file_path.dat");
       assert_eq(1, tarsio_data.fclose.call_count);
       assert_eq((FILE*)0x1234, tarsio_data.fclose.args.arg0);
     }

     test(write_file_shall_not_close_a_file_by_accided_if_file_was_not_opened) {
       write_file("some_file_path.dat");
       assert_eq(0, tarsio_data.fclose.call_count);
     }

10. Compile'n'run

  The this test will not even compile. Since the Tarsio tool-chain did not even
  find any calls to ``fclose()``. Hence the ``tarsio_data`` struct will not
  even contain the member ``fclose`` as sample data for the asserts in the test.

11. Implement the code

   The early exit code style::

     int write_file(const char* filename) {
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         return -1; /* Could not open file for writing */
       }
       fclose(fd);
       return 0; /* Everything is OK */
     }

   or the if/else style::

     int write_file(const char* filename) {
       int retval = 0;
       FILE* fd = NULL;
       fd = fopen(filename, "w")
       if (NULL != fd) {
         fclose(fd);
       }
       else {
         retval = -1;
       }
       return retval; /* Everything is OK */
     }

   or even self-documenting goto style::

     int write_file(const char* filename) {
       int retval = 0;
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         retval = -1;
         goto fopen_failed;
       }
       fclose(fd);
       goto everything_is_ok;
      fopen_failed:
      everything_is_ok:
       return retval;
     }

12. Compile'n'run

   Now there is some error-recovery in place, and also good and understandable
   return values. All tests should still pass.

   A small tip - For free, to have a good self-documenting code style
   regardless of your preferred code aesthetics is to actually name the return
   values to something meaningful, which might be important in the non-goto
   style versions::

     typedef enum {
       WRITE_FILE_EVERYTHING_IS_OK = 0,
       WRITE_FILE_FOPEN_FAILED = -1
     } write_file_rt;

   ... and change the return value type from ``int`` to write_file_rt. And if
   you are clever this can also be used in the test-cases to give them even
   more self-documenting features. It's up to you the coding master.

   In this case the test-cases are refactored first, and then the code, with
   the exact same mind-set as the initial implementation.

13. Write a test that makes sure all data is written do disk.

   Here comes a bit trickier refactoring, along with new implementation. Since
   there is no data passed to the function yet. More arguments have to be
   added, and all existing tests need to be refactored to take these in to
   account. But if things are designed in a good way this should be quite easy
   and in most cases the new arguments can be disregarded completely, since we
   are doing white-box testing and know the program flow (and have it verified
   by the tests written)::

     test(write_file_should_open_the_correct_file_for_writing) {
       write_file("some_file_path.dat", NULL, 0);
       :
       Same as before
       :
     }

     test(write_file_shall_return_negative_1_if_fopen_failes) {
       assert_eq(-1, write_file("some_file_path.dat", NULL, 0));
     }

   This one need to be given some extra thought, since something is probably
   going to be written, if everything is OK. Let's just pass some bogus data
   to the function::

     test(write_file_shall_return_0_if_everything_is_ok) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       assert_eq(0, write_file("some_file_path.dat", (void*)0x5678, 10));
     }

     test(write_file_shall_close_the_correct_file_if_opened) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       write_file("some_file_path.dat", (void*)0x5678, 10);
       :
       Same as before
       :
     }

     test(write_file_shall_not_close_a_file_by_accided_if_file_was_not_opened) {
       write_file("some_file_path.dat", NULL, 0);
       assert_eq(0, tarsio_data.fclose.call_count);
     }

   Also - The new tests for actually writing the data passed to ``write_file()``
   can be written.

   First a small test, to make sure that ``fwrite`` is writing the correct data
   to the correct file, by manipulating the retval of ``fopen()`` as before, to
   get a known value that should be passed to ``fwrite``::

     test(write_file_should_write_the_data_to_the_correct_file) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       write_file("some_file_path.dat", (void*)0x5678, 10);
       assert_eq(1, tarsio_data.fwrite.call_count);
       assert_eq((void*)0x5678, tarsio_data.fwrite.args.arg0);
       assert_eq(10, tarsio_data.fwrite.args.arg1);
       assert_eq(1, tarsio_data.fwrite.args.arg2);
       assert_eq((FILE*)0x1234, tarsio_data.fwrite.args.arg3);
     }

   And obviously it can be good to have a test that makes sure that the code
   will not write anything to somewhere that was never opened::

     test(write_file_should_not_write_data_if_fopen_failed) {
       write_file("some_file_path.dat", NULL, 0);
       assert_eq(0, tarsio_data.fwrite.call_count);
     }

   It is always a good idea also to take error handling into account for new
   code added... So let's also write a few test that makes sure that the
   function return something meaningful if ``fwrite`` should fail - For example
   if a disk breaks during the write or a network file-system is suddenly
   unavailable during the write. This would make the code a bit more robust::

     test(write_file_should_return_negative_2_if_file_write_fails) {
       tarsio_data.fopen.retval = (FILE*)0x1234;
       assert_eq(-2, write_file("some_file_path.dat", (void*)0x5678, 10));
     }

   Again - The Tarsio framework has NULL:ed the retval of fwrite automatically
   so it will return 0 bytes written.

   It is also good to know that the file is closed even if fwrite failed, but
   this is actually already covered in the generic assumption that if a fopen
   is successful, the file should also be closed, always.

14. Compile'n'run

   As you might have figured out. This won't compile, since the function has
   not been refactored to take three arguments yet, nor does it call ``fwrite``.

15. Implement the code

   By adding the new arguments to the data and its size, along with writing the
   data to file in the correct place in your code it would probably look
   something like this:

   The early exit code style::

     int write_file(const char* filename, void* data, size_t size) {
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         return -1; /* Could not open file for writing */
       }
       if (size != fwrite(data, size, 1, fd)) {
         fclose(fd);
         return -2;
       }
       fclose(fd);
       return 0; /* Everything is OK */
     }

   or the if/else style::

     int write_file(const char* filename, void* data, size_t size) {
       int retval = 0;
       FILE* fd = NULL;
       fd = fopen(filename, "w")
       if (NULL != fd) {
         if (size != fwrite(data, size, 1, fd)) {
           retval = -2;
         }
         fclose(fd);
       }
       else {
         retval = -1;
       }
       return retval; /* Everything is OK */
     }

   or even goto style::

     int write_file(const char* filename, void* data, size_t size) {
       int retval = 0;
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         retval = -1;
         goto fopen_failed;
       }
       if (size != fwrite(data, size, 1, fd)) {
         retval = -2;
         goto fwrite_failed;
       }
      fwrite_failed:
       fclose(fd);
       goto everything_is_ok;
      fopen_failed:
      everything_is_ok:
       return retval;
     }

16. Compile'n'run

  Now the ``write_file`` function is fairly well unit-tested, and the design
  of the code was fully driven by the tests that was written before the code.

17. Given the fact that something *could* fail during write, might also
    indicate that even the ``fclose()`` could fail, let's test this too...

    First off, a meaningful return code to distinguish a ``fclose()`` failure
    from other failures would probably be nice::

      test(write_file_shall_return_negative_3_if_file_could_not_be_closed) {
        tarsio_data.fopen.retval = (FILE*)0x1234;
        tarsio_data.fclose.retval = EOF;
        assert_eq(-3, write-file("some_file_path.dat", (void*)0x5678, 10));
      }

   Ok... Now we enter an interesting problem. Some coding styles have multiple
   calls to the fclose. Depending on branch state and such things. In essence,
   we can write a test that makes sure that the code can return -3 regardless
   of which branch we enter, or we just pick a coding style that is most
   generic and the easiest to test.

   It's up to you... The test where fclose is called if fopen was successful
   might not suffice anymore. But if you're clever by refactoring the code you
   may not have to write this test.

18. Compile'n'run

   The test should fail. Since the code has not been implemented yet.

19. Implement the code

    The early exit code style::

     int write_file(const char* filename, void* data, size_t size) {
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         return -1; /* Could not open file for writing */
       }
       if (size != fwrite(data, size, 1, fd)) {
         if (0 != fclose(fd)) {
           return -3;
         }
         return -2;
       }
       if (0 != fclose(fd)) {
         return -3;
       }
       return 0; /* Everything is OK */
     }

   or the if/else style::

     int write_file(const char* filename, void* data, size_t size) {
       int retval = 0;
       FILE* fd = NULL;
       fd = fopen(filename, "w")
       if (NULL != fd) {
         if (size != fwrite(data, size, 1, fd)) {
           retval = -2;
         }
         if (0 != fclose(fd)) {
           retval = -3;
         }
       }
       else {
         retval = -1;
       }
       return retval; /* Everything is OK */
     }

   or even goto style::

     int write_file(const char* filename, void* data, size_t size) {
       int retval = 0;
       FILE* fd;
       if (NULL == (fd = fopen(filename, "w"))) {
         retval = -1;
         goto fopen_failed;
       }
       if (size != fwrite(data, size, 1, fd)) {
         retval = -2;
         goto fwrite_failed;
       }
      fwrite_failed:
       if (0 != fclose(fd)) {
         retval = -3;
       }
       goto everything_is_ok;
      fopen_failed:
      everything_is_ok:
       return retval;
     }

20. Compile'n'run

    Now we have a ready function that fulfills the test-cases and everything
    seems OK.

21. Refactor the code to be more effective than religiously efficient

    In this case, just to reduce duplicated code the remaining design is the
    goto version, not that it is better or worse than anything else. It just
    suited the needs of the test-case this time. Also, there was no real need
    to do a goto for the fopen failure, hence early exit is used there. So
    in order to just fulfill the tests the code can look very different. And
    also thinking about what the code should do in different states makes it
    easier to refactor and restructure it to be readable or efficient,
    depending on the current needs.

    Just sit back, and fiddle with the code - All the tests needed are already
    in place, and you should be able to be creative to maintain test constraints
    while making the code extremely easy to understand::

      int write_file(const char* output_filename, void* data, size_t size) {
        const FILE* fd = fopen(output_filename, "w");

        if (NULL == fd) {
          return WRITE_FILE_FOPEN_FAILED;
        }

        const size_t result = fwrite(data, size, 1, fd);
        const int fwrite_ok = (size == result);

        if (0 != fclose(fd)) {
          return WRITE_FILE_FCLOSE_FAILED;
        }

        if (!fwrite_ok) {
          return WRITE_FILE_FWRITE_FAILED;
        }

        return RETURN_EVERYTHING_IS_OK;
      }

  Even though the code above is not compatible with older compilers, nor is
  it consistent in code style. But it shows ONE end-result that is quite
  compact, while still being easy to read. Also it does comply to all the
  tests we've written.

22. In the best of worlds it should be possible to write some kind of
    acceptance test already from the start. But it will probably not drive the
    design of the code so much... But rather set the functional constraints of
    the code that is to be implemented. In this case it can also help to design
    the API early, to reduce the need to refactor test cases, to that regard.

    But it can also inhibit the design flow a bit. Anyhow, here is an example
    of what that might look like. They might add more value as regression tests
    and or integration tests for different OS:es and platforms, hence they
    could also be written afterwords - depending on your preferences::

      module_test(write_file_should_successfully_write_data_to_disk) {
        const char* data = "0123456789";
        assert_eq(0, write_file("/tmp/foo.dat", data, strlen(data));
        char* result = read_file("/tmp/foo.dat");
        assert_eq(0, strcmp(data, result));
        free(result);
        unlink("/tmp/foo.dat");
      }

      module_test(write_file_should_fail_if_file_can_not_be_opened) {
        const char* data = "0123456789";
        tarsio_data.fopen.retval = NULL;
        assert_eq(-1, write_file("/tmp/foo.dat", data, strlen(data));
      }

      module_test(write_file_should_fail_if_file_can_not_be_written) {
        const char* data = "0123456789";
        tarsio_data.fwrite.retval = 0;
        assert_eq(-2, write_file("/tmp/foo.dat", data, strlen(data));
        unlink("/tmp/foo.dat");
      }

      module_test(write_file_should_fail_if_file_can_not_be_closed) {
        const char* data = "0123456789";
        tarsio_data.fclose.retval = EOF;
        assert_eq(-2, write_file("/tmp/foo.dat", data, strlen(data));
        unlink("/tmp/foo.dat");
      }

   Once these are written you basically have everything needed to do
   the code, that's why it might be a good idea to wait (for THIS example).
   For a couple for reasons. These tests promote an up-front planning of the
   code design - which is not an agile mind-set. Also... They will probably
   let a few design-pit-falls slip by... And you will probably end-up with
   code that is slightly different. It may not be bad, nor good. Just
   different.
